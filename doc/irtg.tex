\documentclass[11pt]{article}

\usepackage{url}
\usepackage{natbib}


\title{The IRTG Tool\\Version 1.0}
\author{Alexander Koller\\University of Potsdam\\\url{koller@ling.uni-potsdam.de}}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\clearpage

\section{Introduction} \label{sec:introduction}

This document describes the IRTG Tool, a system that implements
standard algorithms for interpreted regular tree grammars
(IRTGs). IRTGs were introduced by \cite{koller11} as a joint
generalization of a number of monolingual and synchronous grammar
formalisms, tree transducers, and weighted variants of all these.  We
assume here that the reader is familiar with the theory behind IRTGs
and focus on the use of the IRTG tool itself.

The IRTG tool implements a number of standard data structures and
algorithms for working with IRTGs. These include (monolingual or
synchronous) parsing; extraction of a best parse with the Viterbi
algorithm; decoding; and EM training. The implementation of all these
algorithms is completely independent of the particular algebras into
which the IRTGs are interpreted. The IRTG distribution comes with a
number of predefined algebras, including the string algebra $\Sigma^*$
with binary concatenation. In addition, a user can define their own
algebras by implementing a Java class that implements the
\verb?Algebra? interface. These algebras will then integrate
seamlessly with the rest of the IRTG tool, and can be used in decoding
and parsing.

This documentation is structured as follows. We start with a tutorial
walkthrough that demonstrates the main features of the IRTG tool in
Section~\ref{sec:introduction}.  We will then explain some advanced
uses of the tool in Section~\ref{sec:using}. Section~\ref{sec:algebra}
explains how to implement algebras for use with the IRTG
tool. Section~\ref{sec:conclusion} concludes.


\section{A tutorial walkthrough} \label{sec:tutorial}

The IRTG tool is distributed as a ZIP file containing three things:
\begin{itemize}
\item A Java Jar file with the filename \url{irtg-<version>.jar},
  where \verb?<version>? is the version of the system. This is the
  IRTG tool itself.
\item A subdirectory \verb?examples? which contains various examples
  that we will use below.
\item This documentation as a PDF file.
\end{itemize}

Assuming that you have Java (version 6 or higher) installed on your
system, you can unzip the ZIP file, change into the directory this
creates, and then start the IRTG tool from the command line as
follows:

\begin{verbatim}
$ java -jar irtg-1.0.jar
>
\end{verbatim}

If you run the IRTG tool without any further parameters, it will
start the \emph{shell}, in which you can enter commands for
manipulating IRTGs, which will then be executed. The greater-than sign
\verb?>? is a prompt, which asks you to type a command to be
executed.

You can quit the IRTG shell at any time by typing \verb?quit()? or
Control-D.



\subsection{Loading an IRTG from a file}

You can instruct the shell to load an IRTG grammar from a file as
follows:

\begin{verbatim}
> g = irtg(["examples/scfg.irtg"])
>
\end{verbatim}

The \verb?irtg? function reads an IRTG grammar. Here we tell the shell
to read this grammar from the file \verb?scfg.irtg? in the
\verb?examples? subdirectory, and to assign the resulting IRTG object
to the variable \verb?g?. The square brackets around the file name are
important, because this is what instructs the shell to read from a
file with a given name. Without the square brackets, the shell would
simply take the contents of the shell as the grammar.

Take a moment to look at the file \url{examples/scfg.irtg}. The file
starts with a preamble which specified names for the interpretations
in this IRTG, as well as the algebra for each interpretation. It then
continues with entries such as

\begin{verbatim}
S! -> r1(NP,VP)
  [english] *(?1,?2)
  [german] *(?1,?2)
\end{verbatim}

The first line of each entry is the rule of the RTG $G$ in the center of
the IRTG.  We assume that the RTG is normalized, i.e.\ every rule
contains a single terminal symbol.  Thus the example rule specifies
that we can expand a nonterminal symbol \verb?S? into a node whose
label is the terminal symbol \verb?r1? and whose children are labeled
with the nonterminals \verb?NP? and \verb?VP?.  Equivalently, you can
think of the rule as the rule of a top-down tree automaton, which says
that if we try to accept some node $u$ with label \verb?r1? in state
\verb?S?, then we may assign the children of $u$ the states \verb?NP?
and \verb?VP?.  The exclamation mark behind \verb?S? marks \verb?S? as
the start symbol of the RTG or the initial state of the tree
automaton, respectively.

The RTGs that the IRTG tool can read are restricted in two ways: every
rule may only contain a single terminal symbol (i.e., the RTG is
normalized), and every terminal symbol may only occur in a single
rule.  This allows us to define the values for the homomorphism in a
very compact way.  After each RTG rule, there is one line for each
interpretation which specifies the value of the homomorphism of that
interpretation for the terminal symbol that is processed by the
current grammar rule.  Variables are written as terms of the form
\texttt{?n}, where \verb?n? is a number. Note that if $n$ states occur
on the left-hand side of the grammar rule (i.e., the label has arity
$n$), you can use only the variables \texttt{?1}, \ldots, \texttt{?n}
in the homomorphism term. The example rule specifies that
$h_\mathrm{english}(r_1) = *(x_1,x_2)$ and $h_\mathrm{german}(r_1) =
*(x_1,x_2)$; the symbol * is interpreted as concatenation in the
string algebra.

The example grammar also contains terminal rules of the following
form:

\begin{verbatim}
N -> r11
  [english] telescope
  [german] fernrohr
\end{verbatim}

This rule states that the nonterminal \verb?N? may be expanded into
the terminal symbol \verb?r11?. Notice that \verb?r11?  is a symbol of
arity zero, and therefore there are no states (and no brackets) on the
right-hand side of the grammar rule. The homomorphisms are specified as
before, as $h_\mathrm{english}(r_{11}) = \mathtt{telescope}$ and
$h_\mathrm{german}(r_{11}) = \mathtt{fernrohr}$.


\subsection{Parsing}

Now let's use this grammar for parsing an English string.\footnote{In
  reality, the IRTG tool will print many more rules, all involving the
  state \texttt{q\_FAIL\_}. We have omitted these rules for brevity here.}

\begin{verbatim}
> chart = g.parse(english:"john watches the woman with the telescope")
> chart
P,4-5 -> r12  [1.0]
N,6-7 -> r11 [1.0]
N,3-4 -> r10 [1.0]
Det,5-6 -> r9b [1.0]
Det,2-3 -> r9b  [1.0]
PP,4-7 -> r6(P,4-5, NP,5-7) [1.0]
NP,0-1 -> r7 [1.0]
V,1-2 -> r8 [1.0]
Det,5-6 -> r9 [1.0]
Det,2-3 -> r9 [1.0]
S,0-4 -> r1(NP,0-1, VP,1-4) [1.0]
S,0-7! -> r1(NP,0-1, VP,1-7) [1.0]
NP,5-7 -> r2(Det,5-6, N,6-7) [1.0]
NP,2-4 -> r2(Det,2-3, N,3-4) [1.0]
NP,2-7 -> r2(Det,2-3, N,3-7) [1.0]
N,3-7 -> r3(N,3-4, PP,4-7) [1.0]
VP,1-4 -> r4(V,1-2, NP,2-4) [1.0]
VP,1-7 -> r4(V,1-2, NP,2-7) [1.0]
VP,1-7 -> r5(VP,1-4, PP,4-7) [1.0]
\end{verbatim}

Several things happened here. First, we called the \verb?parse? method
of the object stored in the variable \verb?g? to parse the sentence
``John watches the woman with the telescope'' on the \verb?english?
interpretation. This happens by computing the decomposition automaton
$D_w$ for this string in the string algebra and intersecting $G$ with
the homomorphic pre-image $h_\mathrm{english}^{-1}(D_w)$. The result of
this intersection is an RTG $G'$, which we assign to the variable
\verb?chart?. Notice that the values of expressions (such as the
objects stored in a variable) are really Java objects of certain
classes, and have methods that can be called.

Second, we ask the shell to determine the value of the expression
\verb?chart? and print it. \verb?chart? is a variable which contains
an RTG object; printing an RTG amounts to printing all of its rules.
As you can see, the states of $G'$ are pairs of states of $G$
(corresponding to nonterminals of the SCFG) and spans in the input
string. That is, $G'$ is a context-free parse chart of the English
input sentence.

Finally, notice that each rule of \verb?chart? comes with a weight,
printed in square brackets behind the rule. These weights are there
because the IRTG tool works with \emph{weighted} grammars, and $G'$ is
the \emph{weighted} intersection of $G$ and the pre-image. Because we
neither specified weights for the rules of $G$ nor for the pre-image
grammar, all of them default to 1, which is where the 1.0 weights come
from.

We can now ask the IRTG tool to show us all \emph{derivation trees}
(i.e., those trees in $L(G)$ that map to the inputs) of the input
sentence:

\begin{verbatim}
> chart.language()
r1(r7 r5(r4(r8 r2(r9b r10)) r6(r12 r2(r9 r11))))
r1(r7 r4(r8 r2(r9 r3(r10 r6(r12 r2(r9 r11))))))
r1(r7 r5(r4(r8 r2(r9 r10)) r6(r12 r2(r9b r11))))
r1(r7 r4(r8 r2(r9b r3(r10 r6(r12 r2(r9 r11))))))
r1(r7 r5(r4(r8 r2(r9 r10)) r6(r12 r2(r9 r11))))
r1(r7 r4(r8 r2(r9 r3(r10 r6(r12 r2(r9b r11))))))
r1(r7 r5(r4(r8 r2(r9b r10)) r6(r12 r2(r9b r11))))
r1(r7 r4(r8 r2(r9b r3(r10 r6(r12 r2(r9b r11))))))
\end{verbatim}

This calls the \verb?language? method of the weighted RTG. If you
were surprised that the sentence has eight rather than two derivation
trees, congratulations. This is correct, though, because the word
\verb?the? can be generated by two rules, \verb?r9? and \verb?r9b?,
which differ in the German interpretation. The system gets to choose
twice between these two rules, which accounts for the eight parses.

We can also ask the IRTG tool to show us only the highest-scoring
derivation tree:

\begin{verbatim}
> chart.viterbi()
r1(r7 r5(r4(r8 r2(r9 r10)) r6(r12 r2(r9 r11))))
\end{verbatim}

Of course, all derivation trees described by the chart have the same
weight, and so the choice is rather arbitrary here.

\subsection{Decoding}

In addition to parsing, we can use the IRTG tool for synchronous
\emph{decoding}, in which we give the values of some interpretations
and ask for the output values in another interpretation. This assumes
that the homomorphism of the output interpretation is linear.  Here is
an example:

\begin{verbatim}
> g.decode("english", german: "hans betrachtet die frau mit dem fernrohr")
[john, watches, the, woman, with, the, telescope]
\end{verbatim}

This computes $G'' = G \cap h^{-1}_\mathrm{german}(D_{w'})$, where
$w'$ is the German input sentence. It then maps $L(G'')$ into terms
over the English string algebra by computing
$h_\mathrm{english}(L(G''))$, and evaluates these terms in the
algebra. The value of the expression is the set of all of these
values, which are printed line by line. Note that the English string
is not shown as a single string, but as a comma-separated list of
words.  This is because the implementation of the string algebra
represents a string as a list of words internally.

Observe that IRTG grammars are, in principle, fully symmetric, and so
you can use the same grammar to decode from English to German:

\begin{verbatim}
> g.decode("german", english:"john watches the woman with the telescope")
[hans, betrachtet, die, frau, mit, die, fernrohr]
[hans, betrachtet, dem, frau, mit, die, fernrohr]
[hans, betrachtet, die, frau, mit, dem, fernrohr]
[hans, betrachtet, dem, frau, mit, dem, fernrohr]
\end{verbatim}

As above, there are four instead of one strings because the grammar does
not represent case and gender agreement on the German side.


\subsection{Learning IRTG weights}

Algorithms like the Viterbi algorithm we mentioned above are meant to
be used with RTGs that have meaningful weights.  These weights are
typically learned from corpora of inputs that are suitable for the
IRTG, perhaps together with annotations that specify the correct
derivation tree for each input tuple.  The IRTG tool supports several
training algorithms, but none of them have been tested on large data
sets, so they may not be as efficient as necessary.

\paragraph{Supervised training.}
Consider the case of supervised training first.  In this case, each
training instance in a corpus for training an IRTG with $k$
interpretations consists of $k$ lines of text, each of which is the
string representation of the input object on one dimension, plus a
$k+1$-st line for the correct derivation tree that generated these
input objects.  An annotated corpus with $n$ training instances
consists first of a header of $k$ lines and then of $n$ blocks of
$k+1$ lines, one for each training instance.  The header simply lists
the names of the interpretations in some arbitrary order; the IRTG
tool then assumes that the same order is used for the input objects in
each training instance.

Consider the following example. Let's say we have an IRTG \verb?g2?
which looks as follows. It represents a weighted context-free grammar
in which all weights are 1.

\begin{verbatim}
> g2 = irtg(["examples/cfg.irtg"])
> g2.automaton()
r6(P, NP) -> PP  [1.0]
r7 -> NP  [1.0]
r8 -> V  [1.0]
r9 -> Det  [1.0]
r1(NP, VP) -> S  [1.0]!
r12 -> P  [1.0]
r2(Det, N) -> NP  [1.0]
r3(N, PP) -> N  [1.0]
r11 -> N  [1.0]
r10 -> N  [1.0]
r4(V, NP) -> VP  [1.0]
r5(VP, PP) -> VP  [1.0]
\end{verbatim}

An example for the training data is given in the file
\verb?pcfg-annotated-training.txt? in the \verb?examples?
directory. The file looks as follows:

\begin{verbatim}
i
john watches the woman with the telescope
r1(r7,r5(r4(r8, r2(r9,r10)), r6(r12, r2(r9,r11))))
john watches the telescope with the telescope
r1(r7,r5(r4(r8, r2(r9,r11)), r6(r12, r2(r9,r11))))
john watches the telescope with the woman
r1(r7,r5(r4(r8, r2(r9,r11)), r6(r12, r2(r9,r10))))
\end{verbatim}

We can use it to learn the weights for the PCFG \verb?g2? using
maximum likelihood estimation as follows.

\begin{verbatim}
> c1 = g2.readUnannotatedCorpus(["examples/pcfg-annotated-training.txt"])
> g2.mltrain(c1)
> g2.automaton()
PP -> r6(P, NP) [1.0]
NP -> r7 [0.3333333333333333]
V -> r8 [1.0]
S! -> r1(NP, VP) [1.0]
Det -> r9 [1.0]
NP -> r2(Det, N) [0.6666666666666666]
P -> r12 [1.0]
N -> r11 [0.6666666666666666]
N -> r3(N, PP) [0.0]
VP -> r4(V, NP) [0.5]
N -> r10 [0.3333333333333333]
VP -> r5(VP, PP) [0.5]
\end{verbatim}

\paragraph{Unsupervised training.}



We can then learn PCFG weights for this grammar using Expectation
Maximization (EM) training. We use the file
\url{examples/pcfg-training.txt} as training data. Each training
instance consists of several lines, one for each interpretation of the
IRTG. The order of the interpretations is specified at the start of
the file. In the example, there is only a single interpretation,
\verb?i?, so each line (except for the first one) counts as a single
training instance. We can perform EM training as follows:

\begin{verbatim}
> g2.emtrain(["examples/pcfg-training.txt"])
> g2.automaton()
r6(P, NP) -> PP  [1.0]
r7 -> NP  [0.3333333333333333]
r8 -> V  [1.0]
r9 -> Det  [1.0]
r1(NP, VP) -> S  [1.0]!
r12 -> P  [1.0]
r2(Det, N) -> NP  [0.6666666666666667]
r3(N, PP) -> N  [0.04888355312945965]
r11 -> N  [0.8718567429646621]
r10 -> N  [0.07925970390587832]
r4(V, NP) -> VP  [0.5270903186242554]
r5(VP, PP) -> VP  [0.4729096813757445]
\end{verbatim}

(Notice again that we have used square bracket notation to read data
from files instead of using the string value directly, as in the
parsing examples above.)

The EM algorithm has adjusted the rule weights to fit the
observations. \verb?g2? is an ordinary IRTG and can be used in exactly
the same ways as before; the only difference is that we did not read
it directly from a file, but set the weights through training.


\section{Using the IRTG Tool} \label{sec:using}

I will now go through the methods for each class that are available
from the shell.

This section needs to be improved at some point.

\subsection{Main}

The following methods can be called directly as \verb?method()?,
instead of in the form \verb?object.method()?.

A \verb?Reader? can be obtained in the shell with an expression of the
form \verb?"blah"? (in which case the reader returns the string
``blah'') or the form \verb?["blah"]? (in which case the reader
returns the contents of the file with name ``blah'').

A \verb?Map? can be obtained in the shell with a method call of the
form \verb?object.method(a:x, b:y)?. This call will create a map in
which the value of x is stored under the key ``a'' and the value of y
under the key ``b''.

\begin{description}
\item[InterpretedTreeAutomaton irtg(Reader reader)] Returns the IRTG
  described by the reader.
\item[void println(Object x)] Prints the string representation of
  \verb?x?.
\item[void type(Object x)] Prints the Java class of the object to
  which \verb?x? evaluates.
\item[void quit()] Exits the shell.
\end{description}


\subsection{InterpretedTreeAutomaton}

\begin{description}
\item[BottomUpTreeAutomaton automaton()] Returns the RTG in the center
  of the IRTG as a tree automaton.
\item[Interpretation interpretation(Reader label)] Returns the
  interpretation with the given name.
\item[BottomUpAutomaton parse(Map inputs)] Returns the result of
  parsing the given inputs. \verb?inputs? is a map from
  interpretation names to the inputs on these interpretations.
\item[Set decode(Reader outputInterpretation, Map inputs)] Returns the
  result of parsing the given inputs (see \verb?parse?) and mapping
  them to the given output interpretation.
\item[trainEm(Reader reader)] Adjusts the RTG weights from the given
  training data using EM training.
\end{description}

\subsection{BottomUpTreeAutomaton}

\begin{description}
\item[long countTrees()] Returns the number of trees in the (finite)
  language of the tree automaton.
\item[Tree viterbi()] Computes the tree in the language of the tree
  automaton with the highest weight.
\item[Set language()] Returns the language of the tree automaton.
\end{description}




\section{Implementing algebras} \label{sec:algebra}

A crucial ingredient of an IRTG grammar is the algebra. This is the
only part of the grammar which cannot be specified in the grammar
file. The IRTG tool comes with a number of predefined algebras, but
you can (and probably will) also implement your own algebras. You can
use these algebras just like the predefined algebras, by referencing
them in the preamble of an IRTG grammar file. The algorithms for
parsing, training, and decoding will work for your algebra as well. In
fact, it is the key advantage of working with IRTGs that
algebra-specific aspects of parsing are completely separated from the
core parsing engine.

An algebra in the IRTG tool is a Java class that implements the
interface \verb?Algebra?.  \verb?Algebra? is a generic interface,
which means that you need to specify a type parameter.  If your class
implements \verb?Algebra<O>?, this means that the universe of your
algebra contains all values that are represented by the Java type
\verb?O?.  For example, the class \verb?StringAlgebra? implements the
interface \verb?Algebra<List<String>>?, which means that its values
are lists of strings (namely, a list of the words in a sentence).

The interface \verb?Algebra? defines three methods, which your class
must implement. These methods are as follows.

\begin{description}
\item[evaluate] The \verb?evaluate? method evaluates a term of the
  algebra to an element of the algebra. The term is represented as a
  \verb?Tree? object with string labels. (See
  \verb?StringAlgebra#evaluate? for some methods that can be used on
  trees.)
\item[parseString] The \verb?parseString? method parses the string
  representation of an algebra object and returns the algebra
  object. If the objects of your algebra are represented as terms, you
  may be interested in using the method \verb?TermParser#parse? to
  parse the string representation of the term into a \verb?Term?
  object and then evaluating that term further.
\item[decompose] The \verb?decompose? method computes the
  decomposition automaton for the given algebra object $a$. The
  decomposition automaton accepts exactly those terms that evaluate to
  $a$. Typically, your algebra class will define a private nested
  class that is derived from \verb?BottomUpAutomaton?, and return a
  new object of this class as the return value of \verb?decompose?.
\end{description}

Not every algebra class will do something useful in all of these
methods. The other methods are welcome to throw an
\verb?UnsupportedOperationException?.

\begin{itemize}
\item If you are certain that you will never parse an object of your
  algebra, you do not need to implement the \verb?decompose? and
  \verb?parseString?  methods. Because \verb?decompose? is usually the
  most difficult of the three methods to implement, this can be very
  convenient.
\item If you are certain that you will never decode into an object of
  your algebra, you do not need to implement the \verb?evaluate?
  method.
\item You may find that implementing \verb?evaluate? and
  \verb?parseString? can still be useful for writing test cases.
\end{itemize}

When the IRTG tool reads an IRTG grammar file, it looks for the
algebra classes on the Java classpath. This means that your algebra
class must be on the classpath. The most convenient way to do this may
be to package your algebra class into a Jar file of its own (say,
\verb?foo.jar?). You can then instruct Java to load your Jar file when
starting the shell as follows:

\begin{verbatim}
$ java -cp irtg-1.0.jar:foo.jar de.saar.penguin.irtg.shell.Main
\end{verbatim}

Notice that if you use Windows, you must replace the colon (:) by a
semicolon (;).


\section{Conclusion} \label{sec:conclusion}



\bibliographystyle{apalike}
\bibliography{bibliography}

\end{document}
