\documentclass[11pt]{article}

\usepackage{url}
\usepackage{natbib}
\usepackage{color}

\newcommand{\todo}[1]{\textcolor{red}{(#1)}}

\title{The IRTG Tool\\Version 1.0}
\author{Alexander Koller\\University of Potsdam\\\url{koller@ling.uni-potsdam.de}}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\clearpage

\section{Introduction} \label{sec:introduction}

This document describes the IRTG Tool, a system that implements
standard algorithms for interpreted regular tree grammars
(IRTGs). IRTGs were introduced by \cite{koller11} as a joint
generalization of a number of monolingual and synchronous grammar
formalisms, tree transducers, and weighted variants of all these.  We
assume here that the reader is familiar with the theory behind IRTGs
and focus on the use of the IRTG tool itself.

The IRTG tool implements a number of standard data structures and
algorithms for working with IRTGs. These include (monolingual or
synchronous) parsing; extraction of a best parse with the Viterbi
algorithm; decoding; and EM training. The implementation of all these
algorithms is completely independent of the particular algebras into
which the IRTGs are interpreted. The IRTG distribution comes with a
number of predefined algebras, including the string algebra $\Sigma^*$
with binary concatenation. In addition, a user can define their own
algebras by implementing a Java class that implements the
\verb?Algebra? interface. These algebras will then integrate
seamlessly with the rest of the IRTG tool, and can be used in decoding
and parsing.

This documentation is structured as follows. We start with a tutorial
walkthrough that demonstrates the main features of the IRTG tool in
Section~\ref{sec:introduction}.  We will then explain some advanced
uses of the tool in Section~\ref{sec:using}. Section~\ref{sec:algebra}
explains how to implement algebras for use with the IRTG
tool. Section~\ref{sec:conclusion} concludes.


\section{A tutorial walkthrough} \label{sec:tutorial}

The IRTG tool is distributed as a ZIP file containing three things:
\begin{itemize}
\item A Java Jar file with the filename \url{irtg-<version>.jar},
  where \verb?<version>? is the version of the system. This is the
  IRTG tool itself.
\item A subdirectory \verb?examples? which contains various examples
  that we will use below.
\item This documentation as a PDF file.
\end{itemize}

Assuming that you have Java (version 6 or higher) installed on your
system, you can unzip the ZIP file, change into the directory this
creates, and then start the IRTG tool from the command line as
follows:

\begin{verbatim}
$ java -jar irtg-1.0.jar
>
\end{verbatim}

If you run the IRTG tool without any further parameters, it will
start the \emph{shell}, in which you can enter commands for
manipulating IRTGs, which will then be executed. The greater-than sign
\verb?>? is a prompt, which asks you to type a command to be
executed.

You can quit the IRTG shell at any time by typing \verb?quit()? or
Control-D.



\subsection{Loading an IRTG from a file}

You can instruct the shell to load an IRTG grammar from a file as
follows:

\begin{verbatim}
> g = irtg(["examples/scfg.irtg"])
>
\end{verbatim}

The \verb?irtg? function reads an IRTG grammar. Here we tell the shell
to read this grammar from the file \verb?scfg.irtg? in the
\verb?examples? subdirectory, and to assign the resulting IRTG object
to the variable \verb?g?. The square brackets around the file name are
important, because this is what instructs the shell to read from a
file with a given name. Without the square brackets, the shell would
simply take the contents of the string as the grammar.

Take a moment to look at the file \url{examples/scfg.irtg}. The file
starts with a preamble which specified names for the interpretations
in this IRTG, as well as the algebra for each interpretation. It then
continues with entries such as

\begin{verbatim}
S! -> r1(NP,VP)
  [english] *(?1,?2)
  [german] *(?1,?2)
\end{verbatim}

The first line of each entry is the rule of the RTG $G$ in the center of
the IRTG.  We assume that the RTG is normalized, i.e.\ every rule
contains a single terminal symbol.  Thus the example rule specifies
that we can expand a nonterminal symbol \verb?S? into a node whose
label is the terminal symbol \verb?r1? and whose children are labeled
with the nonterminals \verb?NP? and \verb?VP?.  Equivalently, you can
think of the rule as the rule of a top-down tree automaton, which says
that if we try to accept some node $u$ with label \verb?r1? in state
\verb?S?, then we may assign the children of $u$ the states \verb?NP?
and \verb?VP?.  The exclamation mark behind \verb?S? marks \verb?S? as
the start symbol of the RTG or the initial state of the tree
automaton, respectively.

The RTGs that the IRTG tool can read are restricted in two ways: every
rule may only contain a single terminal symbol (i.e., the RTG is
normalized), and every terminal symbol may only occur in a single
rule.  This allows us to define the values for the homomorphism in a
very compact way.  After each RTG rule, there is one line for each
interpretation which specifies the value of the homomorphism of that
interpretation for the terminal symbol that is processed by the
current grammar rule.  Variables are written as terms of the form
\texttt{?n}, where \verb?n? is a number. Note that if $n$ states occur
on the left-hand side of the grammar rule (i.e., the label has arity
$n$), you can use only the variables \texttt{?1}, \ldots, \texttt{?n}
in the homomorphism term. The example rule specifies that
$h_\mathrm{english}(r_1) = *(x_1,x_2)$ and $h_\mathrm{german}(r_1) =
*(x_1,x_2)$; the symbol * is interpreted as concatenation in the
string algebra.

The example grammar also contains terminal rules of the following
form:

\begin{verbatim}
N -> r11
  [english] telescope
  [german] fernrohr
\end{verbatim}

This rule states that the nonterminal \verb?N? may be expanded into
the terminal symbol \verb?r11?. Notice that \verb?r11?  is a symbol of
arity zero, and therefore there are no states (and no brackets) on the
right-hand side of the grammar rule. The homomorphisms are specified as
before, as $h_\mathrm{english}(r_{11}) = \mathtt{telescope}$ and
$h_\mathrm{german}(r_{11}) = \mathtt{fernrohr}$.


\subsection{Parsing}

Now let's use this grammar for parsing an English string.\footnote{In
  reality, the IRTG tool will print many more rules, all involving the
  state \texttt{q\_FAIL\_}. We have omitted these rules for brevity here.}

\begin{verbatim}
> chart = g.parse(english:"john watches the woman with the telescope")
> chart
P,4-5 -> r12  [1.0]
N,6-7 -> r11 [1.0]
N,3-4 -> r10 [1.0]
Det,5-6 -> r9b [1.0]
Det,2-3 -> r9b  [1.0]
PP,4-7 -> r6(P,4-5, NP,5-7) [1.0]
NP,0-1 -> r7 [1.0]
V,1-2 -> r8 [1.0]
Det,5-6 -> r9 [1.0]
Det,2-3 -> r9 [1.0]
S,0-4 -> r1(NP,0-1, VP,1-4) [1.0]
S,0-7! -> r1(NP,0-1, VP,1-7) [1.0]
NP,5-7 -> r2(Det,5-6, N,6-7) [1.0]
NP,2-4 -> r2(Det,2-3, N,3-4) [1.0]
NP,2-7 -> r2(Det,2-3, N,3-7) [1.0]
N,3-7 -> r3(N,3-4, PP,4-7) [1.0]
VP,1-4 -> r4(V,1-2, NP,2-4) [1.0]
VP,1-7 -> r4(V,1-2, NP,2-7) [1.0]
VP,1-7 -> r5(VP,1-4, PP,4-7) [1.0]
\end{verbatim}

Several things happened here. First, we called the \verb?parse? method
of the object stored in the variable \verb?g? to parse the sentence
``John watches the woman with the telescope'' on the \verb?english?
interpretation. This happens by computing the decomposition automaton
$D_w$ for this string in the string algebra and intersecting $G$ with
the homomorphic pre-image $h_\mathrm{english}^{-1}(D_w)$. The result of
this intersection is an RTG $G'$, which we assign to the variable
\verb?chart?. Notice that the values of expressions (such as the
objects stored in a variable) are really Java objects of certain
classes, and have methods that can be called.

Second, we ask the shell to determine the value of the expression
\verb?chart? and print it. \verb?chart? is a variable which contains
an RTG object; printing an RTG amounts to printing all of its rules.
As you can see, the states of $G'$ are pairs of states of $G$
(corresponding to nonterminals of the SCFG) and spans in the input
string. That is, $G'$ is a context-free parse chart of the English
input sentence.

Finally, notice that each rule of \verb?chart? comes with a weight,
printed in square brackets behind the rule. These weights are there
because the IRTG tool works with \emph{weighted} grammars, and $G'$ is
the \emph{weighted} intersection of $G$ and the pre-image. Because we
neither specified weights for the rules of $G$ nor for the pre-image
grammar, all of them default to 1, which is where the 1.0 weights come
from.

We can now ask the IRTG tool to show us all \emph{derivation trees}
(i.e., those trees in $L(G)$ that map to the inputs) of the input
sentence:

\begin{verbatim}
> chart.language()
r1(r7 r5(r4(r8 r2(r9b r10)) r6(r12 r2(r9 r11))))
r1(r7 r4(r8 r2(r9 r3(r10 r6(r12 r2(r9 r11))))))
r1(r7 r5(r4(r8 r2(r9 r10)) r6(r12 r2(r9b r11))))
r1(r7 r4(r8 r2(r9b r3(r10 r6(r12 r2(r9 r11))))))
r1(r7 r5(r4(r8 r2(r9 r10)) r6(r12 r2(r9 r11))))
r1(r7 r4(r8 r2(r9 r3(r10 r6(r12 r2(r9b r11))))))
r1(r7 r5(r4(r8 r2(r9b r10)) r6(r12 r2(r9b r11))))
r1(r7 r4(r8 r2(r9b r3(r10 r6(r12 r2(r9b r11))))))
\end{verbatim}

This calls the \verb?language? method of the weighted RTG. If you
were surprised that the sentence has eight rather than two derivation
trees, congratulations. This is correct, though, because the word
\verb?the? can be generated by two rules, \verb?r9? and \verb?r9b?,
which differ in the German interpretation. The system gets to choose
twice between these two rules, which accounts for the eight parses.

We can also ask the IRTG tool to show us only the highest-scoring
derivation tree:

\begin{verbatim}
> chart.viterbi()
r1(r7 r5(r4(r8 r2(r9 r10)) r6(r12 r2(r9 r11))))
\end{verbatim}

Of course, all derivation trees described by the chart have the same
weight, and so the choice is rather arbitrary here.

\subsection{Decoding}

In addition to parsing, we can use the IRTG tool for synchronous
\emph{decoding}, in which we give the values of some interpretations
and ask for the output values in another interpretation. This assumes
that the homomorphism of the output interpretation is linear.  Here is
an example:

\begin{verbatim}
> g.decode("english", german: "hans betrachtet die frau mit dem fernrohr")
[john, watches, the, woman, with, the, telescope]
\end{verbatim}

This computes $G'' = G \cap h^{-1}_\mathrm{german}(D_{w'})$, where
$w'$ is the German input sentence. It then maps $L(G'')$ into terms
over the English string algebra by computing
$h_\mathrm{english}(L(G''))$, and evaluates these terms in the
algebra. The value of the expression is the set of all of these
values, which are printed line by line. Note that the English string
is not shown as a single string, but as a comma-separated list of
words.  This is because the implementation of the string algebra
represents a string as a list of words internally.

Observe that IRTG grammars are, in principle, fully symmetric, and so
you can use the same grammar to decode from English to German:

\begin{verbatim}
> g.decode("german", english:"john watches the woman with the telescope")
[hans, betrachtet, die, frau, mit, die, fernrohr]
[hans, betrachtet, dem, frau, mit, die, fernrohr]
[hans, betrachtet, die, frau, mit, dem, fernrohr]
[hans, betrachtet, dem, frau, mit, dem, fernrohr]
\end{verbatim}

As above, there are four instead of one strings because the grammar does
not represent case and gender agreement on the German side.


\subsection{Learning IRTG weights}

Algorithms like the Viterbi algorithm we mentioned above are meant to
be used with RTGs that have meaningful weights.  These weights are
typically learned from corpora of inputs that are suitable for the
IRTG, perhaps together with annotations that specify the correct
derivation tree for each input tuple.  The IRTG tool supports several
training algorithms, but none of them have been tested on large data
sets, so they may not be as efficient as necessary.

\paragraph{Supervised training.}
Consider the case of supervised training first.  In this case, each
training instance in a corpus for training an IRTG with $k$
interpretations consists of $k$ lines of text, each of which is the
string representation of the input object on one dimension, plus a
$k+1$-st line for the correct derivation tree that generated these
input objects.  An annotated corpus with $n$ training instances
consists first of a header of $k$ lines and then of $n$ blocks of
$k+1$ lines, one for each training instance.  The header simply lists
the names of the interpretations in some arbitrary order; the IRTG
tool then assumes that the same order is used for the input objects in
each training instance.

Consider the following example. Let's say we have an IRTG \verb?g2?
which looks as follows. There is a single interpretation, whose name
is \verb?i?, and the automaton is shown below. \verb?g2? represents a
weighted context-free grammar in which all weights are 1.

\begin{verbatim}
> g2 = irtg(["examples/cfg.irtg"])
> g2.automaton()
PP -> r6(P, NP) [1.0]
NP -> r7 [1.0]
V -> r8 [1.0]
Det -> r9 [1.0]
S! -> r1(NP, VP) [1.0]
P -> r12 [1.0]
NP -> r2(Det, N) [1.0]
N -> r3(N, PP) [1.0]
N -> r11 [1.0]
N -> r10 [1.0]
VP -> r4(V, NP) [1.0]
VP -> r5(VP, PP) [1.0]
\end{verbatim}

An example for the training data is given in the file
\verb?pcfg-annotated-training.txt? in the \verb?examples?
directory. The file looks as follows:

\begin{verbatim}
i
john watches the woman with the telescope
r1(r7,r5(r4(r8, r2(r9,r10)), r6(r12, r2(r9,r11))))
john watches the telescope with the telescope
r1(r7,r5(r4(r8, r2(r9,r11)), r6(r12, r2(r9,r11))))
john watches the telescope with the woman
r1(r7,r5(r4(r8, r2(r9,r11)), r6(r12, r2(r9,r10))))
\end{verbatim}

We can use it to learn the weights for the PCFG \verb?g2? using
maximum likelihood estimation by first loading the corpus into an
\verb?AnnotatedCorpus? object, \verb?c1?, and then calling the
IRTG's \verb?mltrain? method to perform the training.

\begin{verbatim}
> c1 = g2.readAnnotatedCorpus(["examples/pcfg-annotated-training.txt"])
> g2.mltrain(c1)
> g2.automaton()
PP -> r6(P, NP) [1.0]
NP -> r7 [0.3333333333333333]
V -> r8 [1.0]
S! -> r1(NP, VP) [1.0]
Det -> r9 [1.0]
NP -> r2(Det, N) [0.6666666666666666]
P -> r12 [1.0]
N -> r11 [0.6666666666666666]
N -> r3(N, PP) [0.0]
VP -> r4(V, NP) [0.5]
N -> r10 [0.3333333333333333]
VP -> r5(VP, PP) [0.5]
\end{verbatim}

Notice that while this particular IRTG (and therefore this particular
annotated corpus) only had a single interpretation, exactly the same
methods can be used to train a synchronous IRTG.  Note also that we
used square bracket notation to instruct the shell to read the corpus
from a file.


\paragraph{Unsupervised training.}
If annotations are not available, we can try to estimate IRTG weights
from unannotated training data. An unannotated corpus looks exactly
like the annotated corpora described above, except that each
individual training instance only consists of a block of $k$ lines for
the input objects on each interpretation. As an example, here are the
contents of the file \verb?pcfg-unannotated-training.txt? from the
\verb?examples? directory:

\begin{verbatim}
i
john watches the woman with the telescope
john watches the telescope with the telescope
john watches the telescope with the woman
\end{verbatim}

We can use this unannotated corpus to perform unsupervised estimation
of the rule weights in the grammar.  For instance, we can use the
IRTG's \verb?emtrain? method to perform Expectation Maximiziation (EM)
training.

\begin{verbatim}
> g3 = irtg(["examples/cfg.irtg"])
> c2 = g3.readUnannotatedCorpus(["examples/pcfg-training.txt"])
> g3.emtrain(c2)
> g3.automaton()
PP -> r6(P, NP) [1.0]
NP -> r7 [0.3333333333333333]
V -> r8 [1.0]
Det -> r9 [1.0]
S! -> r1(NP, VP) [1.0]
P -> r12 [1.0]
NP -> r2(Det, N) [0.6666666666666667]
N -> r3(N, PP) [0.04888355312945965]
N -> r11 [0.8718567429646621]
N -> r10 [0.07925970390587832]
VP -> r4(V, NP) [0.5270903186242554]
VP -> r5(VP, PP) [0.4729096813757445]
\end{verbatim}

In addition to EM, the IRTG tool also implements the Variational Bayes
training algorithm described by
\citet{jones12:_seman_parsin_bayes_tree_trans}.  You can perform VB
training by calling \verb?g3.vbtrain(c2)?.  Note that just like in the
supervised case, all of these training algorithms apply directly to
IRTGs with multiple interpretations.



\section{Using the IRTG Tool} \label{sec:using}

The IRTG tool is distributed as a Java Jar file, which you are free to
use in your own program. The API docs can be found at
http://www.ling.uni-potsdam.de/tcl/irtg/apidocs.

There are two recommended ways to use the IRTG tool as an interactive
main program: through the built-in shell, or using the Scala REPL
interpreter.  Each of these provides mechanisms for evaluating
expressions, storing values in variables, and reading data from files.
In general, the built-in shell is a little more convenient to use for
simple things (such as the tutorial above), but the Scala approach
allows you to use arbitrary methods of the IRTG objects, and offers
you the full power of Scala to process your results.

\subsection{The built-in shell}

The built-in shell of the IRTG tool is started by calling the main
class of the Jar file:

\begin{verbatim}
$ java -jar irtg-1.0.jar
> 
\end{verbatim}

Commands in the shell take one of two forms: an expression, e.g.\
\verb?g.automaton()?, or an assignment of an expression to a variable,
e.g.\ \verb?g = irtg(["examples/cfg.irtg"])?.  In the first case, the
shell simply prints the value of the expression, except if the
expression is of type \verb?void? or the result is \verb?null?. In the
second case, the shell executes the assignment and does not print
anything.

Expressions come in various forms, as follows:
\begin{itemize}
\item A \emph{variable} evaluates to its current value. If the
  variable has not been assigned a value yet, the shell prints an
  error message.
\item A \emph{string literal}, of the form \verb?"foo bar"?, evaluates
  to a \verb?StringReader? for the string inside the quotes. 
\item A \emph{file reader literal}, of the form \verb?["cfg.irtg"]?,
  evaluates to a \verb?FileReader? for the file whose name is given
  inside the quotes.
\item \emph{Method calls}, described in detail below. Method
  calls may not be chained; an expression of the form
  \verb?obj.meth1().meth2()? is illegal in the IRTG shell.
\end{itemize}

Note that both string literals and file reader literals evaluate to
subclasses of \verb?Reader?.  Many methods that are meant for use in
the shell take parameters of class \verb?Reader?.  This leaves you
free to choose whether you want to specify the value of the string
directly in the shell, or whether you want to read it from a file.

Method calls are normally of the form \verb?obj.method(arg1,arg2)?,
and evaluate to the value of the given method call on the given object
with the given arguments.  As a special case, the shell provides a
unique object of class \verb?de.up.ling.irtg.shell.Main?, and you
can call the methods of this object without specifying the object,
i.e.\ you simply call \verb?method(arg1,arg2)?.  This is particularly
useful for very general methods such as \verb?println? and
\verb?quit?.

An object's method may only be called from the shell if it has the
annotation \verb?@CallableFromShell?.  The Javadoc of each method
specifies whether that method has this annotation.  As a further
convenience, some methods take arguments of class \verb?Map?.  You can
pass values into this \verb?Map? by providing method arguments of the
form \verb?key:value?, where \verb?key? is some string (without
quotes) and \verb?value? is an arbitrary expression.  An example of
such a call is \verb?irtg.parse(i: "john sleeps")?, where the
\verb?parse? method is passed a map which contains a StringReader for
the string ``john sleeps'' under the key ``i''.

We refer to the Javadoc for more details on the methods that are
available in the shell.  The most important classes to look at are
\verb?Main?, \verb?TreeAutomaton?, and
\verb?InterpretedTreeAutomaton?.


\subsection{The Scala shell}

Alternatively, you can use the Scala shell as a front-end to the IRTG
library. Scala is a functional programming language that compiles into
Java bytecode, so you can access all classes in the IRTG jar file from
a Scala program. Scala also comes with an interactive shell, which
compiles individual Scala expressions and commands and executes
them. This makes Scala a convenient tool for interacting with the IRTG
library.

You can load the IRTG tool, with some Scala-specific extensions, into
a Scala shell by executing the command \verb?bin/irtg-scala-shell?
from your command line.  You can then load an IRTG and use it to
translate a string (as in the tutorial above) as follows:

\begin{verbatim}
scala> val irtg = IrtgParser.parse(file("examples/scfg.irtg"))
irtg: de.up.ling.irtg.InterpretedTreeAutomaton = 
interpretation german: de.up.ling.irtg.algebra.StringAlgebra
interpretation english: de.up.ling.irtg.algebra.StringAlgebra

N -> r3(N, PP) [1.0]
  [german] *(?1,?2)
  [english] *(?1,?2)
(...)

scala> irtg.decode("german", 
            "english" -> "john watches the woman with the telescope")
res0: java.util.Set[java.lang.Object] = 
   [[hans, betrachtet, die, frau, mit, die, fernrohr], 
    [hans, betrachtet, dem, frau, mit, die, fernrohr], 
    [hans, betrachtet, die, frau, mit, dem, fernrohr], 
    [hans, betrachtet, dem, frau, mit, dem, fernrohr]]
\end{verbatim}

The power of the Scala IRTG shell lies in the fact that Scala is a
full-blown programming language, which allows you to evaluate
arbitrary expressions using arbitrary methods of the objects in the
IRTG library. The shell provides a number of methods for convenience:
\begin{itemize}
\item \verb?file(str)? returns a \verb?FileReader? for the file with
  the name \verb?str? (see the first command in the
  example). \verb?fistream(str)? returns a \verb?FileInputStream? for
  the given file name.
\item There is an implicit conversion from strings to
  \verb?StringReader?s, which is applied by Scala wherever necessary
  (e.g.\ in the ``john watches the woman'' string in the second
  command in the example).
\item The implicit conversions between Scala and Java collections from
  \verb?JavaConversions? are imported (so Scala functions could be
  used to iterate over the \verb?Set? in the example).
\item The syntax \verb?key1 -> val1, key2 -> val2? may be used to
  create a map in which the given keys are mapped to the given
  values (as in \verb?"english" -> "john watches ..."? above).
\end{itemize}





\section{Implementing algebras} \label{sec:algebra}

A crucial ingredient of an IRTG grammar is the algebra. This is the
only part of the grammar which cannot be specified in the grammar
file. The IRTG tool comes with a number of predefined algebras (see
the Javadoc), but you can (and probably will) also implement your own
algebras. You can use these algebras just like the predefined
algebras, by referencing them in the preamble of an IRTG grammar
file. The algorithms for parsing, training, and decoding will work for
your algebra as well. In fact, it is the key advantage of working with
IRTGs that algebra-specific aspects of parsing are completely
separated from the core parsing engine.

An algebra in the IRTG tool is a Java class that implements the
interface \verb?Algebra?.  \verb?Algebra? is a generic interface,
which means that you need to specify a type parameter.  If your class
implements \verb?Algebra<O>?, this means that the universe of your
algebra contains all values that are represented by the Java type
\verb?O?.  For example, the class \verb?StringAlgebra? implements the
interface \verb?Algebra<List<String>>?, which means that its values
are lists of strings (namely, a list of the words in a sentence).

The interface \verb?Algebra? defines four methods, which your class
must implement. These methods are as follows.

\begin{description}
\item[evaluate] The \verb?evaluate? method evaluates a term of the
  algebra to an element of the algebra. The term is represented as a
  \verb?Tree? object with string labels.
\item[parseString] The \verb?parseString? method parses the string
  representation of an algebra object and returns the algebra
  object.
\item[getSignature] The \verb?getSignature? method returns the ranked
  signature of your algebra. See the documentation of the class
  \verb?Signature? for details.
\item[decompose] The \verb?decompose? method computes the
  decomposition automaton for the given algebra object $a$. The
  decomposition automaton should accept exactly those terms that
  evaluate to $a$. Typically, your algebra class will define a private
  nested class that is derived from \verb?TreeAutomaton?, and return a
  new object of this class as the return value of \verb?decompose?.
\end{description}

Not every algebra class will do something useful in all of these
methods. The other methods are welcome to throw an
\verb?UnsupportedOperationException?. In particular:

\begin{itemize}
\item If you are certain that you will never parse an object of your
  algebra, you do not need to implement the \verb?decompose? and
  \verb?parseString?  methods. Because \verb?decompose? is usually the
  most difficult of the three methods to implement, this can be very
  convenient.
\item If you are certain that you will never decode into an object of
  your algebra, you do not need to implement the \verb?evaluate?
  method.
\item You may find that implementing \verb?evaluate? and
  \verb?parseString? can still be useful for writing test cases.
\end{itemize}

When the IRTG tool reads an IRTG grammar file, it looks for the
algebra classes on the Java classpath. This means that your algebra
class must be on the classpath. The most convenient way to do this may
be to package your algebra class into a Jar file of its own (say,
\verb?foo.jar?). You can then instruct Java to load your Jar file when
starting the shell as follows:

\begin{verbatim}
$ java -cp irtg-1.0.jar:foo.jar de.up.ling.irtg.shell.Main
\end{verbatim}

Notice that if you use Windows, you must replace the colon (:) by a
semicolon (;).


\section{Conclusion} \label{sec:conclusion}

The IRTG tool is a work in progress. We have tested it extensively,
but so far only on grammars and data of moderate size. If you use the
IRTG tool on larger amounts of data and find inefficiencies, we would
be grateful to hear about it. Our aim is to optimize the
implementations of the core algorithms over next releases, and to add
further functionality.

This documentation is similarly a work in progress. Any questions and
suggestions for improvements are very welcome.


\section{Version history}

\begin{itemize}
\item 1.0, XXXX: Initial public release.
\end{itemize}


\bibliographystyle{apalike}
\bibliography{bibliography}

\end{document}
